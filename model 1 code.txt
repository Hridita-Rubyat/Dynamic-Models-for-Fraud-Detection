# import library and function

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import precision_score, recall_score, average_precision_score, prec 
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from keras import callbacks
import tensorflow as tf
import calendar
import os
import sys
sys.path.append('/nas/pyfrm_sit_2/innovation/fraud/dcpt/model_2023/common_code/functions
from template_functions import calculate_metrics
from metrics import calculator

# Define the final feature list based on the model's features 
feature_list[78 features]

## Load down-sampled data
# in-time
df_it_initial = pd.read_parquet ('hdfs://cecldev/edl/appl/pyfrm/fraud/p2p_rcv/temp/df_mrd_IT_smp')
# add colmn with in-time 
extra_col_list_it=['fraud_tag', 'valid_ind', 'sampling_weight', 'tran_ts']

df_it_combined= df_it_initial[ feature_list + extra_col_list_it].copy() 
df_it_combined [ 'year_month'] = df_it_combined [ 'tran_ts'].dt.to_period('M') 
df_it_combined = df_it_combined.sort_values(['tran_ts']) # sorted by date 

df_oot_combined = df_it_combined.drop([valid_ind], axis =1)


# Model 1: Architecture
def create_model(learning_rate=0.001):
    model = keras.Sequential([
        keras.layers. Input (shape=(78,)),
        keras.layers. Dense (512, activation= 'relu'),
        keras.layers. Dropout (0.4),
        keras.layers. Dense (256, activation= 'relu'),
        keras.layers.Dropout (0.3),
        keras.layers. Dense (128, activation='relu'), keras.layers. Dropout (0.2),
        keras.layers. Dense (1, activation = 'sigmoid')
    ])
    model.compile(
        optimizer-keras. optimizers.Adam (learning_rate-learning_rate),
        loss='binary_crossentropy',
        metrics=['accuracy',
                 tf.keras.metrics.Precision (name='precision'), 
                 tf.keras.metrics. Recall (name='recall'),
                 tf.keras.metrics. AUC (curve='PR', name='auc_pr')]
    return model

earlystopping = callbacks.EarlyStopping (monitor="val_loss",mode="min",patience=10,restore_best_weights=True)


model1_train_results = []
model1_test_results= []

# Initial training
# Define time periods
initial_start = pd.Period('2023-04')
initial_end = pd. Period ('2023-08')

# Model 1: Data Preparation
# Get initial training data (2023)
initial_train_mask =(df_it_combined [ 'year_month'] >= initial_start) & (df_it_combined [ 'year_month'] <= initial_end) 
initial_data = df_it_combined[initial_train_mask] 

# split initial training data (2023 April to August) in train and valid 
initial_df_train= initial_data.loc[(initial_data['valid_ind']==0), :].reset_index(drop=True).copy()
initial_df_train = initial_df_train.drop('valid_ind', axis =1)
initial_df_valid = initial_data.loc[(df_it['valid_ind' ]== 1), :].reset_index(drop=True).copy() 
initial_df_valid = initial_df_valid.drop('valid_ind', axis =1)


# Initial Null imputation
# identify cols with nulls from train data
col_with_nulls = initial_df_train.columns [initial_df_train.isnull().any()].tolist()
# Fit imputer on train data
median_imputer = SimpleImputer (strategy='median')
median_imputer.fit(initial_df_train[col_with_nulls])
# Transform all datasets
#Impute train data
initial_df_train_imputed = initial_df_train.copy()
initial_df_train_imputed [col_with_nulls] = median_imputer.transform(initial_df_train [col_with_nulls]) 

# Impute valid data
initial_df_valid_imputed = initial_df_valid.copy()
initial_df_valid_imputed [col_with_nulls] = median_imputer.transform(initial_df_valid [col_with_nulls]) 

# Impute OOT data
df_oot_combined_imputed= df_oot_combined.copy()
df_oot_combined_imputed [col_with_nulls] = median_imputer.transform(df_oot_combined [col_with_nulls]) 



### split the initial_df into X, y, weight and value
# train
initial_X_train = initial_df_train_imputed.copy()
initial_X_train = initial_X_train.drop(['fraud_tag', 'tran_ts', 'year_month', 'sampling_weight'], axis =1) 
initial_y_train = initial_df_train_imputed ['fraud_tag'].copy()
# valid
initial_X_val = initial_df_valid_imputed.copy()
initial_X_val = initial_X_val.drop(['fraud_tag', 'tran_ts', 'year_month', 'sampling_weight'], axis =1)
initial_y_val = initial_df_valid_imputed ['fraud_tag'].copy()

scaler = StandardScaler()
initial_X_train_scaled = scaler.fit_transform(initial_X_train) 
initial_X_val_scaled = scaler.transform (initial_X_val)

# Model 1: Initial Training
initial_model = create_model()
initial_history= initial_model.fit(
    initial_X_train_scaled, initial_y_train,
    validation_data=(initial_X_val_scaled, initial_y_val),
    epochs= 400,batch_size=2048, 
    verbose=1,
    callbacks=[earlystopping])

# store initial model and weights
models[initial_end] = initial_model  #The full model is stored in the models dictionary with the date as the key
# The weights are extracted using initial_model.get_weights() and stored in previous_weights
previous_weights = initial_model.get_weights()  ## Saves weights from initial training to a variable to use for September

 
# Store training metrics
model1_train_results.append({
    'date':initial_end,
    #'epoch': range (1, len(initial_history.history['loss']) + 1), 
    'train_loss':     initial_history.history['loss'],
    'val_loss':       initial_history.history['val_loss'],
    'train_accuracy': initial_history.history['accuracy'], 
    'val_accuracy':   initial_history.history['val_accuracy'], 
    'train_auc_pr':   initial_history.history['auc_pr'],
    'val_auc_pr':     initial_history.history['val_auc_pr']
})

# Model 1: Plot initial Training Metrics
plt.figure(figsize=(15, 5))
metrics_to_plot = {
    'Loss': ['train_loss', 'val_loss'],
    'Accuracy': ['train_accuracy', 'val_accuracy'],
    'AUC-PR': ['train_auc_pr', 'val_auc_pr']
}

for i, (metric_name, metric_keys) in enumerate (metrics_to_plot.items()): 
    plt.subplot(1, 3, i + 1)
    plt.plot(initial_history.history [metric_keys[0]], label='Train') 
    plt.plot(initial_history.history [metric_keys [1]], label='Validation') 
    plt.title(f'Initial Model 2 - {metric_name} over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel(metric_name)
    plt.legend()
    plt.tight_layout()
    plt.show()


# Model 1: Testing on Rolling Months
start_date = pd.Period('2023-10-01').start_time
current_date = start_date
end_date = pd. Period ('2024-04-30').end_time

# evaluate on rolling windows
while current_date <= end_date:
    test_start = current_date
    test_end = test_start + pd.offesets.MonthEnd(0)
    
    # get test data
    test_mask = (df_oot_combined_imputed [ 'tran_ts'] >= test_start) & (df_oot_combined_imputed [ 'tran_ts'] <= test_end)
    test_data = df_oot_combined_imputed [test_mask]

    if len(test_data) > 0:
        # Prepare test data
        X_test = test_data.copy()
        X_test = test_data.drop(['fraud_tag', 'tran_ts', 'year_month', 'sampling_weight'], axis =1)
        y_test = test_data['fraud_tag'].copy()
        weight_test = test_data['sampling_weight'].copy()
        
        X_test_scaled = scaler.transform(X_test)
        
        # Get predictions
        y_pred_proba = initial_model.predict(X_test_scaled, verbose=0)
        
        # Calculate metrics using calculate_metrics function
        test_metrics = calculate_metrics (
            pred = y_pred_proba.flatten(), # predicted probabilities
            y = y_test,             #true Labels 
            weight weight_test,   # using sampling_weight
            threshold = 0.01,
            metrics = ['KS', 'AUC', 'AUCPR', 'DR', 'FPR'])


        #Store results 
        model1_test_results.append({

           'date': test_end,
           'model_name': "Static",
           'window': f"tetsing Period: {test_start}))",
           'AUC': test_metrics['AUC'],
           'PR-AUC': test_metrics['AUCPR'],
           'DR': test_metrics['DR'],
           'FPR': test_metrics["FPR"]
             })


# Convert training results to DataFrame
model1_train_results_df = pd.DataFrame (model1_train_results)

# Convert test results to DataFrames
model1_test_results_df = pd.DataFrame(model1_test_results)

print("Model 1 (static) Test Metrics for each window:")
print (model1_test_results[['window', 'AUC', 'PR-AUC', 'DR', 'FPR']])

print("\nAverage Test Metrics:")
print (model1_test_results[['window', 'AUC', 'PR-AUC', 'DR', 'FPR']).mean())


---ModeL 1: Plot Test Metrics-----
plt.figure(figsize=(15, 15))
metrics = ['AUC', 'PR-AUC', 'DR', 'FPR']
for i, metric in enumerate(metrics):
    plt.subplot(4, 2, i+1)
    plt.plot(range(len(model1_test_results_df)), model1_test_results_df[metric], label='Static Model', color='blue')
    plt.title(f"{metric} Over Test Windows")
    plt.xlabel('Test Window')
    plt.ylabel(metric)
    plt.grid(True, linestyle="--", alpha=0.7)
    plt.legend()
    
    # Use window Labels for x-axis
    plt.xticks(range(len(model1_test_results_df)),
        model3_test_results_df['window'],
        rotation=90, ha='right')
plt.tight_layout()
plt.show()



