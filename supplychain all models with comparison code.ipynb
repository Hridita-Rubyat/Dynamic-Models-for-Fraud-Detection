{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa6a705",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Model 1 \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define time periods\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m initial_start \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2015-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m initial_end \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2015-12-31\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m training_end \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2017-11-30\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Model 1 \n",
    "\n",
    "# Define time periods\n",
    "initial_start = pd.Timestamp('2015-01-01')\n",
    "initial_end = pd.Timestamp('2015-12-31')\n",
    "training_end = pd.Timestamp('2017-11-30')\n",
    "eval_end = pd.Timestamp('2018-01-31')\n",
    "\n",
    "# -------------- Model 1: Data Preparation --------------\n",
    "# Get initial training data (2015)\n",
    "initial_data = df[df['order_date'] <= initial_end]\n",
    "X_initial = initial_data.drop(columns=['fraud', 'order_date'])\n",
    "y_initial = initial_data['fraud']\n",
    "\n",
    "# Split and scale initial data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_initial, y_initial, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# -------------- Model 1: Architecture --------------\n",
    "def create_model(learning_rate=0.001):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(39,)),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(16, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy',\n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(curve='PR', name='auc_pr')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -------------- Model 1: Training --------------\n",
    "print(f\"\\nTraining Model 1 (Static) with data up to {initial_end.strftime('%Y-%m-%d')}...\")\n",
    "\n",
    "model1 = create_model()\n",
    "history1 = model1.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Store training metrics\n",
    "model1_train_results = pd.DataFrame({\n",
    "    'epoch': range(1, len(history1.history['loss']) + 1),\n",
    "    'loss': history1.history['loss'],\n",
    "    'val_loss': history1.history['val_loss'],\n",
    "    'accuracy': history1.history['accuracy'],\n",
    "    'val_accuracy': history1.history['val_accuracy'],\n",
    "    'auc_pr': history1.history['auc_pr'],\n",
    "    'val_auc_pr': history1.history['val_auc_pr']\n",
    "})\n",
    "\n",
    "# -------------- Model 1: Testing on Rolling Windows --------------\n",
    "# Initialize results storage\n",
    "model1_results = []\n",
    "\n",
    "# Test window parameters\n",
    "last_valid_start = pd.Timestamp('2017-12-01')\n",
    "current_date = pd.Timestamp('2016-02-01')\n",
    "\n",
    "# Evaluate on rolling windows\n",
    "while current_date <= last_valid_start:\n",
    "    # Define test window\n",
    "    test_start = current_date\n",
    "    test_end_month = current_date.month + 1\n",
    "    test_end_year = current_date.year\n",
    "    if test_end_month > 12:\n",
    "        test_end_month -= 12\n",
    "        test_end_year += 1\n",
    "    \n",
    "    last_day = calendar.monthrange(test_end_year, test_end_month)[1]\n",
    "    test_end = pd.Timestamp(f\"{test_end_year}-{test_end_month:02d}-{last_day:02d}\")\n",
    "    \n",
    "    # Get test data\n",
    "    test_mask = (df['order_date'] > test_start) & (df['order_date'] <= test_end)\n",
    "    test_data = df[test_mask]\n",
    "    \n",
    "    if len(test_data) > 0:\n",
    "        # Prepare test data\n",
    "        X_test = test_data.drop(columns=['fraud', 'order_date'])\n",
    "        y_test = test_data['fraud']\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred_proba = model1.predict(X_test_scaled, verbose=0)\n",
    "        threshold = np.quantile(y_pred_proba, 0.99)  # Top 1%\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        print(f\"Threshold value for this window: {threshold}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        precisions, recalls, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        pr_auc = auc(recalls, precisions)\n",
    "        \n",
    "        # Store results\n",
    "        model1_results.append({\n",
    "            'date': current_date,\n",
    "            'model_name': 'Static',  # Changed from 'model' to 'model_name' for consistency\n",
    "            'window': f\"{test_start.strftime('%Y-%m')} to {test_end.strftime('%Y-%m')}\",\n",
    "            'PR-AUC': pr_auc,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision,\n",
    "            'Recall': recall\n",
    "        })\n",
    "    \n",
    "    current_date += pd.DateOffset(months=1)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "model1_test_results = pd.DataFrame(model1_results)\n",
    "\n",
    "# Store final results for comparison\n",
    "model1_final_results = {\n",
    "    'model_name': 'Static',\n",
    "    'train_metrics': model1_train_results,\n",
    "    'test_metrics': model1_test_results\n",
    "}\n",
    "\n",
    "# Save results for later comparison\n",
    "np.save('model1_results.npy', model1_final_results, allow_pickle=True)\n",
    "\n",
    "# Print summary metrics\n",
    "print(\"\\nModel 1 (Static) Test Metrics:\")\n",
    "print(model1_test_results[['window', 'PR-AUC', 'Accuracy', 'Precision', 'Recall']])\n",
    "print(\"\\nAverage Test Metrics:\")\n",
    "print(model1_test_results[['PR-AUC', 'Accuracy', 'Precision', 'Recall']].mean())\n",
    "\n",
    "\n",
    "\n",
    "# -------------- Model 1: Plot Training Metrics --------------\n",
    "plt.figure(figsize=(15, 5))\n",
    "metrics_to_plot = {\n",
    "    'Loss': ['loss', 'val_loss'],\n",
    "    'Accuracy': ['accuracy', 'val_accuracy'],\n",
    "    'AUC-PR': ['auc_pr', 'val_auc_pr']\n",
    "}\n",
    "\n",
    "for i, (metric_name, metric_keys) in enumerate(metrics_to_plot.items()):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.plot(history1.history[metric_keys[0]], label='Train')\n",
    "    plt.plot(history1.history[metric_keys[1]], label='Validation')\n",
    "    plt.title(f'Model 1 - {metric_name} over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -------------- Model 1: Plot Test Metrics --------------\n",
    "plt.figure(figsize=(15, 10))\n",
    "metrics = ['PR-AUC', 'Accuracy', 'Precision', 'Recall']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(range(len(model1_test_results)), model1_test_results[metric], label='Static Model', color='blue')\n",
    "    plt.title(f'{metric} Over Test Windows')\n",
    "    plt.xlabel('Test Window')\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Use window labels for x-axis\n",
    "    plt.xticks(range(len(model1_test_results)), \n",
    "               model1_test_results['window'], \n",
    "               rotation=90,\n",
    "               ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model 2\n",
    "\n",
    "# Define time periods\n",
    "initial_start = pd.Timestamp('2015-01-01')\n",
    "initial_end = pd.Timestamp('2015-12-31')\n",
    "training_end = pd.Timestamp('2017-11-30')\n",
    "eval_end = pd.Timestamp('2018-01-31')\n",
    "\n",
    "def create_model(learning_rate=0.001):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(39,)),\n",
    "        keras.layers.Dense(64, activation='relu', name='dense_1'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(32, activation='relu', name='dense_2'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(16, activation='relu', name='dense_3'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(1, activation='sigmoid', name='output')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy',\n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(curve='PR', name='auc_pr')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -------------- Model 2: Initial Training (2015) --------------\n",
    "print(\"\\nStarting Model 2: Initial Training on 2015 data...\")\n",
    "\n",
    "# Initialize storage\n",
    "models = {}\n",
    "#scalers = {}\n",
    "model2_train_results = []\n",
    "model2_results = []\n",
    "\n",
    "# Get 2015 data\n",
    "initial_mask = (df['order_date'] >= initial_start) & (df['order_date'] <= initial_end)\n",
    "initial_data = df[initial_mask]\n",
    "\n",
    "# Prepare 2015 data\n",
    "X_initial = initial_data.drop(columns=['fraud', 'order_date'])\n",
    "y_initial = initial_data['fraud']\n",
    "\n",
    "# Split 2015 data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_initial, y_initial,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Scale 2015 data\n",
    "initial_scaler = StandardScaler()\n",
    "X_train_scaled = initial_scaler.fit_transform(X_train)\n",
    "X_val_scaled = initial_scaler.transform(X_val)\n",
    "\n",
    "# Train 2015 model\n",
    "initial_model = create_model(learning_rate=0.001)\n",
    "initial_history = initial_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Store 2015 model and scaler\n",
    "models[initial_end] = initial_model\n",
    "#scalers[initial_end] = initial_scaler\n",
    "previous_weights = initial_model.get_weights()\n",
    "\n",
    "# Store 2015 training metrics\n",
    "model2_train_results.append({\n",
    "    'date': initial_end,\n",
    "    'train_loss': initial_history.history['loss'][-1],\n",
    "    'val_loss': initial_history.history['val_loss'][-1],\n",
    "    'train_accuracy': initial_history.history['accuracy'][-1],\n",
    "    'val_accuracy': initial_history.history['val_accuracy'][-1],\n",
    "    'train_auc_pr': initial_history.history['auc_pr'][-1],\n",
    "    'val_auc_pr': initial_history.history['val_auc_pr'][-1]\n",
    "})\n",
    "\n",
    "# -------------- Model 2: Monthly Updates (2016-2017) --------------\n",
    "print(\"\\nStarting Model 2: Monthly Updates...\")\n",
    "\n",
    "current_date = pd.Timestamp('2016-01-31')  # First update month\n",
    "last_train_date = pd.Timestamp('2017-11-30')  # Last update month\n",
    "\n",
    "while current_date <= last_train_date:\n",
    "    print(f\"\\nProcessing month ending {current_date.strftime('%Y-%m-%d')}...\")\n",
    "    \n",
    "    # Get current month's training data\n",
    "    month_start = current_date.replace(day=1)  # First day of current month\n",
    "    month_end = (month_start + pd.DateOffset(months=1) - pd.DateOffset(days=1))  # Last day of current month\n",
    "    \n",
    "    train_mask = (df['order_date'] > month_start) & (df['order_date'] <= month_end)\n",
    "    train_data = df[train_mask]\n",
    "    print(f\"Training data period: {month_start.strftime('%Y-%m-%d')} to {month_end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Training data size: {len(train_data)}\")\n",
    "    \n",
    "    # Prepare current month's training data\n",
    "    X_train = train_data.drop(columns=['fraud', 'order_date'])\n",
    "    y_train = train_data['fraud']\n",
    "    \n",
    "    # Split current month's data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale current month's data\n",
    "    X_train_scaled = initial_scaler.transform(X_train)\n",
    "    X_val_scaled = initial_scaler.transform(X_val)\n",
    "    \n",
    "    # Create new model with previous weights\n",
    "    current_model = create_model(learning_rate=0.0001)\n",
    "    current_model.set_weights(previous_weights)\n",
    "    \n",
    "    # Train on current month\n",
    "    history = current_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=3,\n",
    "        batch_size=256,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Store model, scaler, and weights\n",
    "    models[current_date] = current_model\n",
    "    #scalers[current_date] = current_scaler\n",
    "    previous_weights = current_model.get_weights()\n",
    "    print(f\"Storing weights from {current_date.strftime('%Y-%m')} model for next month's training\")\n",
    "    \n",
    "    # Store training metrics\n",
    "    model2_train_results.append({\n",
    "        'date': current_date,\n",
    "        'train_loss': history.history['loss'][-1],\n",
    "        'val_loss': history.history['val_loss'][-1],\n",
    "        'train_accuracy': history.history['accuracy'][-1],\n",
    "        'val_accuracy': history.history['val_accuracy'][-1],\n",
    "        'train_auc_pr': history.history['auc_pr'][-1],\n",
    "        'val_auc_pr': history.history['val_auc_pr'][-1]\n",
    "    })\n",
    "    \n",
    "    # -------------- Testing on Next Two Months --------------\n",
    "    # Define test window\n",
    "    test_start = current_date + pd.DateOffset(days=1)\n",
    "    test_end = test_start + pd.DateOffset(months=2) - pd.DateOffset(days=1)\n",
    "    \n",
    "    test_mask = (df['order_date'] >= test_start) & (df['order_date'] <= test_end)\n",
    "    test_data = df[test_mask]\n",
    "    \n",
    "    print(f\"Testing period: {test_start.strftime('%Y-%m-%d')} to {test_end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Test data size: {len(test_data)}\")\n",
    "    \n",
    "    if len(test_data) > 0:\n",
    "        # Prepare and scale test data using current month's scaler\n",
    "        X_test = test_data.drop(columns=['fraud', 'order_date'])\n",
    "        y_test = test_data['fraud']\n",
    "        X_test_scaled = initial_scaler.transform(X_test)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred_proba = current_model.predict(X_test_scaled, verbose=0)\n",
    "        # With:\n",
    "        threshold = np.quantile(y_pred_proba, 0.99)  # Top 1%\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        print(f\"Threshold value for this window: {threshold}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        precisions, recalls, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        pr_auc = auc(recalls, precisions)\n",
    "        \n",
    "        # Store results\n",
    "        model2_results.append({\n",
    "            'date': current_date,\n",
    "            'model': 'Incremental',\n",
    "            'window': f\"{test_start.strftime('%Y-%m')} to {test_end.strftime('%Y-%m')}\",\n",
    "            'PR-AUC': pr_auc,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'threshold': threshold\n",
    "        })\n",
    "    \n",
    "    # Move to next month\n",
    "    current_date = (current_date + pd.DateOffset(months=1))\n",
    "\n",
    "# -------------- Results Analysis --------------\n",
    "# Convert results to DataFrames\n",
    "model2_train_results_df = pd.DataFrame(model2_train_results)\n",
    "model2_results_df = pd.DataFrame(model2_results)\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(model2_train_results_df)\n",
    "\n",
    "print(\"\\nTest Metrics for each window:\")\n",
    "print(model2_results_df[['window', 'PR-AUC', 'Accuracy', 'Precision', 'Recall']])\n",
    "\n",
    "print(\"\\nAverage Test Metrics:\")\n",
    "print(model2_results_df[['PR-AUC', 'Accuracy', 'Precision', 'Recall']].mean())\n",
    "\n",
    "# Plot metrics over time\n",
    "plt.figure(figsize=(15, 10))\n",
    "metrics = ['PR-AUC', 'Accuracy', 'Precision', 'Recall']\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(range(len(model2_results_df)), model2_results_df[metric], \n",
    "             label='Incremental Model', color='green')\n",
    "    plt.title(f'{metric} Over Time')\n",
    "    plt.xlabel('Test Windows')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.xticks(range(len(model2_results_df)), \n",
    "               model2_results_df['window'], \n",
    "               rotation=90,\n",
    "               ha='right')\n",
    "    \n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "model2_train_results_df.to_csv('model2_training_metrics.csv', index=False)\n",
    "model2_results_df.to_csv('model2_test_metrics.csv', index=False)\n",
    "\n",
    "# Store final results for comparison\n",
    "model2_final_results = {\n",
    "    'model_name': 'Incremental',\n",
    "    'train_metrics': model2_train_results_df,\n",
    "    'test_metrics': model2_results_df\n",
    "}\n",
    "\n",
    "# Save results for later comparison\n",
    "np.save('model2_results.npy', model2_final_results, allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Model 3 \n",
    "\n",
    "# Define time periods\n",
    "initial_start = pd.Timestamp('2015-01-01')\n",
    "initial_end = pd.Timestamp('2015-12-31')\n",
    "training_end = pd.Timestamp('2017-11-30')\n",
    "eval_end = pd.Timestamp('2018-01-31')\n",
    "\n",
    "def create_model(learning_rate=0.001):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(39,)),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(16, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy',\n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(curve='PR', name='auc_pr')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# -------------- Model 3: Dynamic Full train Model (Training) --------------\n",
    "print(\"\\nStarting Model 3 (Dynamic Full_train - Training)...\")\n",
    "\n",
    "# Initialize storage for models, scaler, and results\n",
    "models = {}\n",
    "scaler = StandardScaler()\n",
    "model3_train_results = []\n",
    "training_histories = []  # For storing training histories\n",
    "\n",
    "# Set up training loop\n",
    "current_date = pd.Timestamp('2016-02-01')  # Start from Feb 2016\n",
    "last_valid_start = pd.Timestamp('2017-12-01')  # End at Dec 2017\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "while current_date <= last_valid_start:\n",
    "    # Define training window (accumulated data up to previous month end)\n",
    "    train_end = current_date - pd.DateOffset(days=1)\n",
    "    train_start = pd.Timestamp('2015-01-01')\n",
    "  \n",
    "    # Get accumulated training data\n",
    "    train_mask = (df['order_date'] >= train_start) & (df['order_date'] <= train_end)\n",
    "    train_data = df[train_mask]\n",
    "    print(f\"\\nRetraining Model 3 for data up to {train_end.strftime('%Y-%m-%d')}...\")\n",
    "    print(f\"Training data size: {len(train_data)}\")\n",
    "\n",
    "    # Prepare training data\n",
    "    X_train = train_data.drop(columns=['fraud', 'order_date'])\n",
    "    y_train = train_data['fraud']\n",
    "\n",
    "\n",
    "    # Split and scale\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "      X_train, y_train,\n",
    "      test_size=0.2,\n",
    "      random_state=42\n",
    "    )\n",
    "\n",
    "    # Use initial scaler to all\n",
    "    if current_date == pd.Timestamp('2016-02-01'):\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "    else:\n",
    "       X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    current_model = create_model()\n",
    "    history = current_model.fit(\n",
    "      X_train_scaled, y_train,\n",
    "      validation_data=(X_val_scaled, y_val),\n",
    "      epochs=10,\n",
    "      batch_size=512,\n",
    "      verbose=1\n",
    "    )\n",
    "\n",
    "    models[train_end] = current_model\n",
    "    training_histories.append({\n",
    "      'window_end': train_end,\n",
    "      'history': history\n",
    "    })\n",
    "\n",
    "    model3_train_results.append({\n",
    "      'date': train_end,\n",
    "      'train_loss': history.history['loss'][-1],\n",
    "      'val_loss': history.history['val_loss'][-1],\n",
    "      'train_accuracy': history.history['accuracy'][-1],\n",
    "      'val_accuracy': history.history['val_accuracy'][-1],\n",
    "      'train_auc_pr': history.history['auc_pr'][-1],\n",
    "      'val_auc_pr': history.history['val_auc_pr'][-1]\n",
    "    })\n",
    "\n",
    "    current_date += pd.DateOffset(months=1)\n",
    "\n",
    "# Convert training results to DataFrame\n",
    "model3_train_results_df = pd.DataFrame(model3_train_results)\n",
    "print(\"\\nTraining Metrics (Model 3):\")\n",
    "print(model3_train_results_df)\n",
    "\n",
    "\n",
    "\n",
    "# -------------- Model 3: Testing on Rolling Windows --------------\n",
    "print(\"\\nStarting Model 3 Testing...\")\n",
    "\n",
    "# Testing Loop\n",
    "model3_results = []\n",
    "current_date = pd.Timestamp('2016-02-01')\n",
    "\n",
    "while current_date <= last_valid_start:\n",
    "    test_start = current_date\n",
    "    test_end_month = current_date.month + 1\n",
    "    test_end_year = current_date.year\n",
    "    if test_end_month > 12:\n",
    "        test_end_month -= 12\n",
    "        test_end_year += 1\n",
    "\n",
    "    last_day = calendar.monthrange(test_end_year, test_end_month)[1]\n",
    "    test_end = pd.Timestamp(f\"{test_end_year}-{test_end_month:02d}-{last_day:02d}\")\n",
    "\n",
    "    train_end = current_date - pd.DateOffset(days=1)\n",
    "    current_model = models[train_end]\n",
    "\n",
    "    test_mask = (df['order_date'] > test_start) & (df['order_date'] <= test_end)\n",
    "    test_data = df[test_mask]\n",
    "\n",
    "    if len(test_data) > 0:\n",
    "      X_test = test_data.drop(columns=['fraud', 'order_date'])\n",
    "      y_test = test_data['fraud']\n",
    "      X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "      y_pred_proba = current_model.predict(X_test_scaled, verbose=0)\n",
    "      threshold = np.quantile(y_pred_proba, 0.99)  # Top 1%\n",
    "      y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "      print(f\"Threshold value for this window: {threshold}\")\n",
    "\n",
    "      precision = precision_score(y_test, y_pred)\n",
    "      recall = recall_score(y_test, y_pred)\n",
    "      precisions, recalls, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "      pr_auc = auc(recalls, precisions)\n",
    "\n",
    "      model3_results.append({\n",
    "          'date': current_date,\n",
    "          'model': 'Full_train',\n",
    "          'window': f\"{test_start.strftime('%Y-%m')} to {test_end.strftime('%Y-%m')}\",\n",
    "          'PR-AUC': pr_auc,\n",
    "          'Accuracy': accuracy_score(y_test, y_pred),\n",
    "          'Precision': precision,   \n",
    "          'Recall': recall,\n",
    "          'threshold': threshold\n",
    "      })\n",
    "\n",
    "    current_date += pd.DateOffset(months=1)\n",
    "\n",
    "# Print results\n",
    "model3_results_df = pd.DataFrame(model3_results)\n",
    "print(\"\\nMetrics for each test window (Full_train Model):\")\n",
    "print(model3_results_df[['window', 'PR-AUC', 'Accuracy', 'Precision', 'Recall']])\n",
    "\n",
    "print(\"\\nAverage metrics (Full_train Model):\")\n",
    "average_metrics = model3_results_df[['PR-AUC', 'Accuracy', 'Precision', 'Recall']].mean()\n",
    "print(average_metrics)\n",
    "\n",
    "# Plot test metrics\n",
    "metrics = ['PR-AUC', 'Accuracy', 'Precision', 'Recall']\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "   plt.subplot(2, 2, i + 1)\n",
    "   plt.plot(range(len(model3_results_df)), model3_results_df[metric], \n",
    "            label='Full_train Model', color='blue')\n",
    "   plt.title(f'{metric} Over Time')\n",
    "   plt.xlabel('Test Windows')\n",
    "   plt.ylabel(metric)\n",
    "   plt.legend()\n",
    "   plt.xticks(range(len(model3_results_df)), \n",
    "              model3_results_df['window'], \n",
    "              rotation=90,\n",
    "              ha='right')\n",
    "   plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "model3_train_results_df.to_csv('model3_Full_train Model_training_metrics.csv', index=False)\n",
    "model3_results_df.to_csv('model3_Full_train Model_test_metrics.csv', index=False)\n",
    "\n",
    "# Store final results for comparison\n",
    "model3_final_results = {\n",
    "    'model_name': 'Full_train Model',\n",
    "    'train_metrics': model3_train_results_df,\n",
    "    'test_metrics': model3_results_df\n",
    "}\n",
    "\n",
    "# Save results for later comparison\n",
    "np.save('model3_results.npy', model3_final_results, allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load results\n",
    "model1_results = np.load('model1_results.npy', allow_pickle=True).item()\n",
    "model2_results = np.load('model2_results.npy', allow_pickle=True).item()\n",
    "model3_results = np.load('model3_results.npy', allow_pickle=True).item()\n",
    "\n",
    "# Create a dictionary of test metrics for each model\n",
    "test_metrics = {\n",
    "    'Static': model1_results['test_metrics'],\n",
    "    'Incremental': model2_results['test_metrics'],\n",
    "    'Full Training': model3_results['test_metrics']\n",
    "}\n",
    "\n",
    "# Calculate average metrics for each model\n",
    "avg_metrics = {}\n",
    "for model_name, metrics in test_metrics.items():\n",
    "    avg_metrics[model_name] = {\n",
    "        'PR-AUC': metrics['PR-AUC'].mean(),\n",
    "        'Accuracy': metrics['Accuracy'].mean(),\n",
    "        'Precision': metrics['Precision'].mean(),\n",
    "        'Recall': metrics['Recall'].mean()\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "avg_metrics_df = pd.DataFrame(avg_metrics).round(4)\n",
    "print(\"\\nAverage Metrics Across All Test Windows:\")\n",
    "print(avg_metrics_df)\n",
    "\n",
    "# Print detailed metrics for each window\n",
    "print(\"\\nDetailed Metrics for Each Test Window:\")\n",
    "metrics = ['PR-AUC', 'Accuracy', 'Precision', 'Recall']\n",
    "\n",
    "for window_idx in range(len(test_metrics['Static'])):\n",
    "    print(f\"\\nWindow {window_idx + 1}: {test_metrics['Static'].iloc[window_idx]['window']}\")\n",
    "    window_metrics = {}\n",
    "    for model_name, results in test_metrics.items():\n",
    "        window_metrics[model_name] = {\n",
    "            'PR-AUC': results.iloc[window_idx]['PR-AUC'],\n",
    "            'Accuracy': results.iloc[window_idx]['Accuracy'],\n",
    "            'Precision': results.iloc[window_idx]['Precision'],\n",
    "            'Recall': results.iloc[window_idx]['Recall']\n",
    "        }\n",
    "    window_df = pd.DataFrame(window_metrics).round(4)\n",
    "    print(window_df)\n",
    "\n",
    "# Find best performing model for each metric\n",
    "best_models = {}\n",
    "for metric in metrics:\n",
    "    best_model = max(avg_metrics.items(), key=lambda x: x[1][metric])[0]\n",
    "    best_models[metric] = best_model\n",
    "\n",
    "print(\"\\nBest Performing Models:\")\n",
    "for metric, model in best_models.items():\n",
    "    print(f\"{metric}: {model} ({avg_metrics[model][metric]:.4f})\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Load results\n",
    "model1_results = np.load('model1_results.npy', allow_pickle=True).item()\n",
    "model2_results = np.load('model2_results.npy', allow_pickle=True).item()\n",
    "model3_results = np.load('model3_results.npy', allow_pickle=True).item()\n",
    "\n",
    "# Create dictionary of test metrics\n",
    "test_metrics = {\n",
    "    'Static': model1_results['test_metrics'],\n",
    "    'Incremental': model2_results['test_metrics'],\n",
    "    'Full Training': model3_results['test_metrics']\n",
    "}\n",
    "\n",
    "# Plot metrics over time\n",
    "plt.figure(figsize=(20, 15))\n",
    "metrics = ['PR-AUC', 'Accuracy', 'Precision', 'Recall']\n",
    "colors = ['blue', 'green', 'orange']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    \n",
    "    for j, (model_name, results) in enumerate(test_metrics.items()):\n",
    "        plt.plot(range(len(results)), results[metric], \n",
    "                label=model_name, color=colors[j], linewidth=2)\n",
    "    \n",
    "    plt.title(f'{metric} Over Time', fontsize=14, pad=20)\n",
    "    plt.xlabel('Test Windows', fontsize=12)\n",
    "    plt.ylabel(metric, fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Format x-axis labels\n",
    "    window_labels = results['window'].tolist()\n",
    "    plt.xticks(range(len(window_labels)), window_labels, \n",
    "              rotation=90, ha='right', fontsize=8)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.gca().set_xticklabels(window_labels)\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Adjust spacing\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()\n",
    "\n",
    "# Optionally save the plot\n",
    "plt.savefig('model_comparison_plots.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4ad35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35896d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
